{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set necessary libraries\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import sqlite3 as sq\n",
    "from sqlalchemy import create_engine\n",
    "from flask import jsonify\n",
    "import config\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://jincgnemrccala:8d89aa468b157c4377d9a9488cc121f42fd08b5efca2bcef03dab6a9aedf09df@ec2-54-235-86-101.compute-1.amazonaws.com:5432/d8eoiti175ci0a\n"
     ]
    }
   ],
   "source": [
    "#we need to set our database credentials\n",
    "# if another user is using file, they will need to create their own config.py file locally with address to postgres db\n",
    "    \n",
    "print(config.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create engine\n",
    "engine = sqlalchemy.create_engine(config.url)\n",
    "\n",
    "#connect to our server\n",
    "\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to read our csv files\n",
    "combined_file = \"data/Combined_Data.csv\"\n",
    "gdp_file = \"data/GDP_Data.csv\"\n",
    "population_file= \"data/Population_Data.csv\"\n",
    "netmigration_file = \"data/Net_Migration_Data.csv\"\n",
    "life_file = \"data/Life_Expectancy_Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for combined\n",
    "combined_df = pd.read_csv(combined_file)\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for gdp\n",
    "gdp_df = pd.read_csv(gdp_file)\n",
    "gdp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for net migration\n",
    "netmigration_df = pd.read_csv(netmigration_file)\n",
    "netmigration_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for population\n",
    "pop_df = pd.read_csv(population_file)\n",
    "pop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for life expectancy\n",
    "life_df = pd.read_csv(life_file)\n",
    "life_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to upload our combined dataframe to Posgres SQL table in Heroku\n",
    "combined_df.to_sql(name=\"world_indices\",con=engine, if_exists = 'replace', index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to add a primary key to our file, otherwise our automap_base() will not work\n",
    "with engine.connect() as con:\n",
    "    con.execute('ALTER TABLE world_indices ADD PRIMARY KEY (id);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to upload our gdp dataframe to Posgres SQL table in Heroku\n",
    "gdp_df.to_sql(name=\"world_gdp\",con=engine, if_exists = 'replace', index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to add a primary key to our file, otherwise our automap_base() will not work\n",
    "with engine.connect() as con:\n",
    "    con.execute('ALTER TABLE world_gdp ADD PRIMARY KEY (id);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to upload our netmigration dataframe to Posgres SQL table in Heroku\n",
    "netmigration_df.to_sql(name=\"world_netmigration\",con=engine, if_exists = 'replace', index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to add a primary key to our file, otherwise our automap_base() will not work\n",
    "with engine.connect() as con:\n",
    "    con.execute('ALTER TABLE world_netmigration ADD PRIMARY KEY (id);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to upload our  dataframe to Posgres SQL table in Heroku\n",
    "pop_df.to_sql(name=\"world_population\",con=engine, if_exists = 'replace', index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to add a primary key to our file, otherwise our automap_base() will not work\n",
    "with engine.connect() as con:\n",
    "    con.execute('ALTER TABLE world_population ADD PRIMARY KEY (id);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to upload our  dataframe to Posgres SQL table in Heroku\n",
    "life_df.to_sql(name=\"life_table\",con=engine, if_exists = 'replace', index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to add a primary key to our file, otherwise our automap_base() will not work\n",
    "with engine.connect() as con:\n",
    "    con.execute('ALTER TABLE life_table ADD PRIMARY KEY (id);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read our data\n",
    "netmigration_table = pd.read_sql_table(table_name='world_netmigration', con=engine)\n",
    "netmigration_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check datatypes and see that the column years are objects \n",
    "# netmigration_table.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list comprehension for the range for values, need to make sure range captures all the years\n",
    "years_columns =[str(x) for x in (range(1960,2018))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas pivot to make better organize our original data\n",
    "netmigration_pivot = pd.pivot_table(netmigration_table, values=years_columns, index= ['Country Name'], columns=[\"Series Name\"])\n",
    "netmigration_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use code below to create new columns that combine multi index\n",
    "final_netcolumns = [' '.join(col).strip() for col in netmigration_pivot.columns.values]\n",
    "# final_netcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set our new table columns to the final columns created above, and set to json\n",
    "netmigration_pivot.columns= final_netcolumns\n",
    "netmigration_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json data for net migration\n",
    "netmigration_pivot.reset_index().to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read our data\n",
    "gdp_table = pd.read_sql_table(table_name='world_gdp', con=engine)\n",
    "gdp_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas pivot to make better organize our original data\n",
    "gdp_pivot = pd.pivot_table(gdp_table, values=years_columns, index= ['Country Name'], columns=[\"Series Name\"])\n",
    "gdp_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use code below to create new columns that combine multi index\n",
    "final_gdpcolumns = [' '.join(col).strip() for col in gdp_pivot.columns.values]\n",
    "# final_gdpcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set our new table columns to the final columns created above, and set to json\n",
    "gdp_pivot.columns= final_gdpcolumns\n",
    "gdp_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json data for gdp pivot\n",
    "gdp_pivot.reset_index().to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read our data\n",
    "combined_table = pd.read_sql_table(table_name='world_indices', con=engine)\n",
    "combined_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_table.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list comprehension for the range for values, need to make sure range captures all the years\n",
    "years_columns =[str(x) for x in (range(1960,2018))]\n",
    "# years_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas pivot to make better organize our original data\n",
    "new_combinedtable = pd.pivot_table(combined_table, values=years_columns, index= ['CountryName'], columns=[\"Index\"])\n",
    "new_combinedtable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use code below to create new columns that combine multi index\n",
    "final_columns = [' '.join(col).strip() for col in new_combinedtable.columns.values]\n",
    "# final_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set our new table columns to the final columns created above, and set to json\n",
    "new_combinedtable.columns= final_columns\n",
    "new_combinedtable.head()\n",
    "# return new_table.reset_index().to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_combinedtable.reset_index().to_json(orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
